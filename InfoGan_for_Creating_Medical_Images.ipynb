{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDfoHK7bQ3KwmE71aIuOLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewjonathanbs/Computer_Vision_Kalbe/blob/main/InfoGan_for_Creating_Medical_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eriklindernoren/PyTorch-GAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwAjZcR-xkEt",
        "outputId": "f3768c2f-cf20-4d6e-b8f6-9fde646abbc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorch-GAN'...\n",
            "remote: Enumerating objects: 1283, done.\u001b[K\n",
            "remote: Total 1283 (delta 0), reused 0 (delta 0), pack-reused 1283\u001b[K\n",
            "Receiving objects: 100% (1283/1283), 68.04 MiB | 34.41 MiB/s, done.\n",
            "Resolving deltas: 100% (751/751), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib numpy scipy pillow urllib3 scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9de-pvTqxu2O",
        "outputId": "43282136-bd8b-454d-ebb9-79d4301df55f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "XMvLh_0FvkdX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkvNVvadvlI8",
        "outputId": "4a8e43ab-f5b0-4f6a-8bce-c5e6569d0f1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah46gjdRuL-7",
        "outputId": "c3563c65-dc36-4358-8e0d-e1abd65b365e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2689c52fa01e>:169: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  static_z = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.latent_dim))))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/200] [Batch 0/18] [D loss: 0.466205] [G loss: 0.931462] [info loss: 2.341027]\n",
            "[Epoch 0/200] [Batch 1/18] [D loss: 0.464096] [G loss: 0.927450] [info loss: 2.332062]\n",
            "[Epoch 0/200] [Batch 2/18] [D loss: 0.462187] [G loss: 0.923967] [info loss: 2.338529]\n",
            "[Epoch 0/200] [Batch 3/18] [D loss: 0.460506] [G loss: 0.920429] [info loss: 2.331097]\n",
            "[Epoch 0/200] [Batch 4/18] [D loss: 0.457997] [G loss: 0.916056] [info loss: 2.331408]\n",
            "[Epoch 0/200] [Batch 5/18] [D loss: 0.455492] [G loss: 0.912981] [info loss: 2.332094]\n",
            "[Epoch 0/200] [Batch 6/18] [D loss: 0.453391] [G loss: 0.908245] [info loss: 2.336673]\n",
            "[Epoch 0/200] [Batch 7/18] [D loss: 0.450264] [G loss: 0.903636] [info loss: 2.336754]\n",
            "[Epoch 0/200] [Batch 8/18] [D loss: 0.446852] [G loss: 0.898139] [info loss: 2.337358]\n",
            "[Epoch 0/200] [Batch 9/18] [D loss: 0.443358] [G loss: 0.892123] [info loss: 2.335482]\n",
            "[Epoch 0/200] [Batch 10/18] [D loss: 0.436928] [G loss: 0.883504] [info loss: 2.332258]\n",
            "[Epoch 0/200] [Batch 11/18] [D loss: 0.431253] [G loss: 0.873761] [info loss: 2.331432]\n",
            "[Epoch 0/200] [Batch 12/18] [D loss: 0.427710] [G loss: 0.856450] [info loss: 2.339617]\n",
            "[Epoch 0/200] [Batch 13/18] [D loss: 0.419265] [G loss: 0.836933] [info loss: 2.336098]\n",
            "[Epoch 0/200] [Batch 14/18] [D loss: 0.412115] [G loss: 0.808583] [info loss: 2.334291]\n",
            "[Epoch 0/200] [Batch 15/18] [D loss: 0.399610] [G loss: 0.781902] [info loss: 2.334296]\n",
            "[Epoch 0/200] [Batch 16/18] [D loss: 0.385892] [G loss: 0.729878] [info loss: 2.334864]\n",
            "[Epoch 0/200] [Batch 17/18] [D loss: 0.378624] [G loss: 0.690130] [info loss: 2.332712]\n",
            "[Epoch 1/200] [Batch 0/18] [D loss: 0.367144] [G loss: 0.591198] [info loss: 2.333487]\n",
            "[Epoch 1/200] [Batch 1/18] [D loss: 0.371810] [G loss: 0.545077] [info loss: 2.334322]\n",
            "[Epoch 1/200] [Batch 2/18] [D loss: 0.339754] [G loss: 0.441817] [info loss: 2.335408]\n",
            "[Epoch 1/200] [Batch 3/18] [D loss: 0.350720] [G loss: 0.429976] [info loss: 2.334362]\n",
            "[Epoch 1/200] [Batch 4/18] [D loss: 0.362112] [G loss: 0.351333] [info loss: 2.333348]\n",
            "[Epoch 1/200] [Batch 5/18] [D loss: 0.348077] [G loss: 0.342993] [info loss: 2.334999]\n",
            "[Epoch 1/200] [Batch 6/18] [D loss: 0.373250] [G loss: 0.350072] [info loss: 2.333327]\n",
            "[Epoch 1/200] [Batch 7/18] [D loss: 0.352118] [G loss: 0.336640] [info loss: 2.331914]\n",
            "[Epoch 1/200] [Batch 8/18] [D loss: 0.340256] [G loss: 0.357692] [info loss: 2.334102]\n",
            "[Epoch 1/200] [Batch 9/18] [D loss: 0.328812] [G loss: 0.367096] [info loss: 2.329444]\n",
            "[Epoch 1/200] [Batch 10/18] [D loss: 0.340444] [G loss: 0.307671] [info loss: 2.332672]\n",
            "[Epoch 1/200] [Batch 11/18] [D loss: 0.332009] [G loss: 0.333675] [info loss: 2.331893]\n",
            "[Epoch 1/200] [Batch 12/18] [D loss: 0.340509] [G loss: 0.317584] [info loss: 2.331179]\n",
            "[Epoch 1/200] [Batch 13/18] [D loss: 0.336337] [G loss: 0.321653] [info loss: 2.331794]\n",
            "[Epoch 1/200] [Batch 14/18] [D loss: 0.313820] [G loss: 0.336590] [info loss: 2.334612]\n",
            "[Epoch 1/200] [Batch 15/18] [D loss: 0.311704] [G loss: 0.354459] [info loss: 2.322905]\n",
            "[Epoch 1/200] [Batch 16/18] [D loss: 0.303180] [G loss: 0.301399] [info loss: 2.328580]\n",
            "[Epoch 1/200] [Batch 17/18] [D loss: 0.329608] [G loss: 0.294588] [info loss: 2.327315]\n",
            "[Epoch 2/200] [Batch 0/18] [D loss: 0.304850] [G loss: 0.283916] [info loss: 2.322279]\n",
            "[Epoch 2/200] [Batch 1/18] [D loss: 0.315953] [G loss: 0.307665] [info loss: 2.324940]\n",
            "[Epoch 2/200] [Batch 2/18] [D loss: 0.298555] [G loss: 0.335244] [info loss: 2.318378]\n",
            "[Epoch 2/200] [Batch 3/18] [D loss: 0.284172] [G loss: 0.282299] [info loss: 2.324091]\n",
            "[Epoch 2/200] [Batch 4/18] [D loss: 0.288636] [G loss: 0.302904] [info loss: 2.313796]\n",
            "[Epoch 2/200] [Batch 5/18] [D loss: 0.286017] [G loss: 0.282720] [info loss: 2.314490]\n",
            "[Epoch 2/200] [Batch 6/18] [D loss: 0.306455] [G loss: 0.278508] [info loss: 2.313764]\n",
            "[Epoch 2/200] [Batch 7/18] [D loss: 0.278772] [G loss: 0.279189] [info loss: 2.310703]\n",
            "[Epoch 2/200] [Batch 8/18] [D loss: 0.258672] [G loss: 0.323505] [info loss: 2.314661]\n",
            "[Epoch 2/200] [Batch 9/18] [D loss: 0.249503] [G loss: 0.314228] [info loss: 2.304570]\n",
            "[Epoch 2/200] [Batch 10/18] [D loss: 0.283321] [G loss: 0.344858] [info loss: 2.305917]\n",
            "[Epoch 2/200] [Batch 11/18] [D loss: 0.275740] [G loss: 0.367190] [info loss: 2.298031]\n",
            "[Epoch 2/200] [Batch 12/18] [D loss: 0.271262] [G loss: 0.339754] [info loss: 2.295856]\n",
            "[Epoch 2/200] [Batch 13/18] [D loss: 0.274173] [G loss: 0.309924] [info loss: 2.289438]\n",
            "[Epoch 2/200] [Batch 14/18] [D loss: 0.266920] [G loss: 0.219066] [info loss: 2.297930]\n",
            "[Epoch 2/200] [Batch 15/18] [D loss: 0.247177] [G loss: 0.314486] [info loss: 2.288529]\n",
            "[Epoch 2/200] [Batch 16/18] [D loss: 0.269995] [G loss: 0.291832] [info loss: 2.288115]\n",
            "[Epoch 2/200] [Batch 17/18] [D loss: 0.235980] [G loss: 0.292601] [info loss: 2.278895]\n",
            "[Epoch 3/200] [Batch 0/18] [D loss: 0.244042] [G loss: 0.326761] [info loss: 2.267234]\n",
            "[Epoch 3/200] [Batch 1/18] [D loss: 0.240765] [G loss: 0.341433] [info loss: 2.268216]\n",
            "[Epoch 3/200] [Batch 2/18] [D loss: 0.254949] [G loss: 0.249061] [info loss: 2.250638]\n",
            "[Epoch 3/200] [Batch 3/18] [D loss: 0.244568] [G loss: 0.299250] [info loss: 2.235656]\n",
            "[Epoch 3/200] [Batch 4/18] [D loss: 0.263434] [G loss: 0.257661] [info loss: 2.238820]\n",
            "[Epoch 3/200] [Batch 5/18] [D loss: 0.225716] [G loss: 0.355815] [info loss: 2.230689]\n",
            "[Epoch 3/200] [Batch 6/18] [D loss: 0.241793] [G loss: 0.375700] [info loss: 2.193827]\n",
            "[Epoch 3/200] [Batch 7/18] [D loss: 0.261280] [G loss: 0.337770] [info loss: 2.180416]\n",
            "[Epoch 3/200] [Batch 8/18] [D loss: 0.188911] [G loss: 0.423503] [info loss: 2.180894]\n",
            "[Epoch 3/200] [Batch 9/18] [D loss: 0.211619] [G loss: 0.454461] [info loss: 2.159492]\n",
            "[Epoch 3/200] [Batch 10/18] [D loss: 0.209498] [G loss: 0.325234] [info loss: 2.147011]\n",
            "[Epoch 3/200] [Batch 11/18] [D loss: 0.211889] [G loss: 0.393977] [info loss: 2.070720]\n",
            "[Epoch 3/200] [Batch 12/18] [D loss: 0.200507] [G loss: 0.425809] [info loss: 2.035941]\n",
            "[Epoch 3/200] [Batch 13/18] [D loss: 0.177698] [G loss: 0.358456] [info loss: 2.073395]\n",
            "[Epoch 3/200] [Batch 14/18] [D loss: 0.218350] [G loss: 0.412335] [info loss: 2.019980]\n",
            "[Epoch 3/200] [Batch 15/18] [D loss: 0.225109] [G loss: 0.438950] [info loss: 2.019130]\n",
            "[Epoch 3/200] [Batch 16/18] [D loss: 0.258800] [G loss: 0.444624] [info loss: 1.964342]\n",
            "[Epoch 3/200] [Batch 17/18] [D loss: 0.250157] [G loss: 0.400410] [info loss: 1.994847]\n",
            "[Epoch 4/200] [Batch 0/18] [D loss: 0.218051] [G loss: 0.448356] [info loss: 1.885278]\n",
            "[Epoch 4/200] [Batch 1/18] [D loss: 0.223956] [G loss: 0.481972] [info loss: 1.857418]\n",
            "[Epoch 4/200] [Batch 2/18] [D loss: 0.202786] [G loss: 0.429700] [info loss: 1.908292]\n",
            "[Epoch 4/200] [Batch 3/18] [D loss: 0.181633] [G loss: 0.422141] [info loss: 1.866123]\n",
            "[Epoch 4/200] [Batch 4/18] [D loss: 0.241426] [G loss: 0.550340] [info loss: 1.861489]\n",
            "[Epoch 4/200] [Batch 5/18] [D loss: 0.226402] [G loss: 0.518391] [info loss: 1.918271]\n",
            "[Epoch 4/200] [Batch 6/18] [D loss: 0.191820] [G loss: 0.649934] [info loss: 1.856936]\n",
            "[Epoch 4/200] [Batch 7/18] [D loss: 0.231963] [G loss: 0.402842] [info loss: 1.872045]\n",
            "[Epoch 4/200] [Batch 8/18] [D loss: 0.285583] [G loss: 0.451014] [info loss: 1.832323]\n",
            "[Epoch 4/200] [Batch 9/18] [D loss: 0.234562] [G loss: 0.587824] [info loss: 1.812702]\n",
            "[Epoch 4/200] [Batch 10/18] [D loss: 0.233916] [G loss: 0.770773] [info loss: 1.775806]\n",
            "[Epoch 4/200] [Batch 11/18] [D loss: 0.212378] [G loss: 0.527034] [info loss: 1.800853]\n",
            "[Epoch 4/200] [Batch 12/18] [D loss: 0.205054] [G loss: 0.500186] [info loss: 1.789051]\n",
            "[Epoch 4/200] [Batch 13/18] [D loss: 0.249240] [G loss: 0.454576] [info loss: 1.758496]\n",
            "[Epoch 4/200] [Batch 14/18] [D loss: 0.212694] [G loss: 0.549155] [info loss: 1.735440]\n",
            "[Epoch 4/200] [Batch 15/18] [D loss: 0.223725] [G loss: 0.614977] [info loss: 1.733152]\n",
            "[Epoch 4/200] [Batch 16/18] [D loss: 0.227265] [G loss: 0.428890] [info loss: 1.723749]\n",
            "[Epoch 4/200] [Batch 17/18] [D loss: 0.265282] [G loss: 0.403016] [info loss: 1.708308]\n",
            "[Epoch 5/200] [Batch 0/18] [D loss: 0.245019] [G loss: 0.524520] [info loss: 1.674138]\n",
            "[Epoch 5/200] [Batch 1/18] [D loss: 0.241224] [G loss: 0.503017] [info loss: 1.696394]\n",
            "[Epoch 5/200] [Batch 2/18] [D loss: 0.209503] [G loss: 0.452837] [info loss: 1.710109]\n",
            "[Epoch 5/200] [Batch 3/18] [D loss: 0.222771] [G loss: 0.495866] [info loss: 1.664060]\n",
            "[Epoch 5/200] [Batch 4/18] [D loss: 0.218132] [G loss: 0.413948] [info loss: 1.646763]\n",
            "[Epoch 5/200] [Batch 5/18] [D loss: 0.228339] [G loss: 0.447151] [info loss: 1.655646]\n",
            "[Epoch 5/200] [Batch 6/18] [D loss: 0.240328] [G loss: 0.389721] [info loss: 1.647403]\n",
            "[Epoch 5/200] [Batch 7/18] [D loss: 0.235474] [G loss: 0.573937] [info loss: 1.616588]\n",
            "[Epoch 5/200] [Batch 8/18] [D loss: 0.258178] [G loss: 0.483415] [info loss: 1.677563]\n",
            "[Epoch 5/200] [Batch 9/18] [D loss: 0.213653] [G loss: 0.478651] [info loss: 1.627532]\n",
            "[Epoch 5/200] [Batch 10/18] [D loss: 0.298005] [G loss: 0.362959] [info loss: 1.640661]\n",
            "[Epoch 5/200] [Batch 11/18] [D loss: 0.255868] [G loss: 0.408866] [info loss: 1.648913]\n",
            "[Epoch 5/200] [Batch 12/18] [D loss: 0.236187] [G loss: 0.391528] [info loss: 1.614124]\n",
            "[Epoch 5/200] [Batch 13/18] [D loss: 0.236523] [G loss: 0.338036] [info loss: 1.632488]\n",
            "[Epoch 5/200] [Batch 14/18] [D loss: 0.267334] [G loss: 0.471982] [info loss: 1.607971]\n",
            "[Epoch 5/200] [Batch 15/18] [D loss: 0.256304] [G loss: 0.387369] [info loss: 1.593489]\n",
            "[Epoch 5/200] [Batch 16/18] [D loss: 0.274097] [G loss: 0.343028] [info loss: 1.628407]\n",
            "[Epoch 5/200] [Batch 17/18] [D loss: 0.297330] [G loss: 0.289203] [info loss: 1.578326]\n",
            "[Epoch 6/200] [Batch 0/18] [D loss: 0.269985] [G loss: 0.475695] [info loss: 1.597870]\n",
            "[Epoch 6/200] [Batch 1/18] [D loss: 0.308320] [G loss: 0.325747] [info loss: 1.617994]\n",
            "[Epoch 6/200] [Batch 2/18] [D loss: 0.289331] [G loss: 0.388000] [info loss: 1.595207]\n",
            "[Epoch 6/200] [Batch 3/18] [D loss: 0.266212] [G loss: 0.463716] [info loss: 1.637204]\n",
            "[Epoch 6/200] [Batch 4/18] [D loss: 0.236120] [G loss: 0.404921] [info loss: 1.576534]\n",
            "[Epoch 6/200] [Batch 5/18] [D loss: 0.246183] [G loss: 0.362758] [info loss: 1.567487]\n",
            "[Epoch 6/200] [Batch 6/18] [D loss: 0.316478] [G loss: 0.375873] [info loss: 1.578622]\n",
            "[Epoch 6/200] [Batch 7/18] [D loss: 0.331228] [G loss: 0.401109] [info loss: 1.573541]\n",
            "[Epoch 6/200] [Batch 8/18] [D loss: 0.288129] [G loss: 0.385148] [info loss: 1.566307]\n",
            "[Epoch 6/200] [Batch 9/18] [D loss: 0.379298] [G loss: 0.355404] [info loss: 1.580675]\n",
            "[Epoch 6/200] [Batch 10/18] [D loss: 0.296994] [G loss: 0.381229] [info loss: 1.591244]\n",
            "[Epoch 6/200] [Batch 11/18] [D loss: 0.267055] [G loss: 0.433343] [info loss: 1.583751]\n",
            "[Epoch 6/200] [Batch 12/18] [D loss: 0.271372] [G loss: 0.349292] [info loss: 1.581851]\n",
            "[Epoch 6/200] [Batch 13/18] [D loss: 0.275429] [G loss: 0.460252] [info loss: 1.621438]\n",
            "[Epoch 6/200] [Batch 14/18] [D loss: 0.331881] [G loss: 0.435738] [info loss: 1.563121]\n",
            "[Epoch 6/200] [Batch 15/18] [D loss: 0.339054] [G loss: 0.535133] [info loss: 1.601029]\n",
            "[Epoch 6/200] [Batch 16/18] [D loss: 0.308871] [G loss: 0.348439] [info loss: 1.569923]\n",
            "[Epoch 6/200] [Batch 17/18] [D loss: 0.254980] [G loss: 0.393230] [info loss: 1.571161]\n",
            "[Epoch 7/200] [Batch 0/18] [D loss: 0.280888] [G loss: 0.304485] [info loss: 1.581143]\n",
            "[Epoch 7/200] [Batch 1/18] [D loss: 0.268581] [G loss: 0.412099] [info loss: 1.562665]\n",
            "[Epoch 7/200] [Batch 2/18] [D loss: 0.281593] [G loss: 0.435214] [info loss: 1.581421]\n",
            "[Epoch 7/200] [Batch 3/18] [D loss: 0.277065] [G loss: 0.432937] [info loss: 1.586612]\n",
            "[Epoch 7/200] [Batch 4/18] [D loss: 0.288427] [G loss: 0.314404] [info loss: 1.547689]\n",
            "[Epoch 7/200] [Batch 5/18] [D loss: 0.333514] [G loss: 0.333510] [info loss: 1.544936]\n",
            "[Epoch 7/200] [Batch 6/18] [D loss: 0.289301] [G loss: 0.407413] [info loss: 1.559751]\n",
            "[Epoch 7/200] [Batch 7/18] [D loss: 0.346534] [G loss: 0.377629] [info loss: 1.544263]\n",
            "[Epoch 7/200] [Batch 8/18] [D loss: 0.331971] [G loss: 0.402721] [info loss: 1.587422]\n",
            "[Epoch 7/200] [Batch 9/18] [D loss: 0.281258] [G loss: 0.427084] [info loss: 1.552293]\n",
            "[Epoch 7/200] [Batch 10/18] [D loss: 0.277531] [G loss: 0.430170] [info loss: 1.560946]\n",
            "[Epoch 7/200] [Batch 11/18] [D loss: 0.302038] [G loss: 0.348541] [info loss: 1.572373]\n",
            "[Epoch 7/200] [Batch 12/18] [D loss: 0.338700] [G loss: 0.372876] [info loss: 1.550665]\n",
            "[Epoch 7/200] [Batch 13/18] [D loss: 0.245786] [G loss: 0.445098] [info loss: 1.555257]\n",
            "[Epoch 7/200] [Batch 14/18] [D loss: 0.307970] [G loss: 0.311228] [info loss: 1.537926]\n",
            "[Epoch 7/200] [Batch 15/18] [D loss: 0.307401] [G loss: 0.379712] [info loss: 1.539069]\n",
            "[Epoch 7/200] [Batch 16/18] [D loss: 0.305029] [G loss: 0.358780] [info loss: 1.566346]\n",
            "[Epoch 7/200] [Batch 17/18] [D loss: 0.301652] [G loss: 0.273241] [info loss: 1.539998]\n",
            "[Epoch 8/200] [Batch 0/18] [D loss: 0.278753] [G loss: 0.469276] [info loss: 1.531864]\n",
            "[Epoch 8/200] [Batch 1/18] [D loss: 0.273599] [G loss: 0.368728] [info loss: 1.527191]\n",
            "[Epoch 8/200] [Batch 2/18] [D loss: 0.304899] [G loss: 0.414494] [info loss: 1.538524]\n",
            "[Epoch 8/200] [Batch 3/18] [D loss: 0.304624] [G loss: 0.372280] [info loss: 1.543471]\n",
            "[Epoch 8/200] [Batch 4/18] [D loss: 0.296620] [G loss: 0.335585] [info loss: 1.536250]\n",
            "[Epoch 8/200] [Batch 5/18] [D loss: 0.284433] [G loss: 0.344613] [info loss: 1.539643]\n",
            "[Epoch 8/200] [Batch 6/18] [D loss: 0.275596] [G loss: 0.339960] [info loss: 1.533875]\n",
            "[Epoch 8/200] [Batch 7/18] [D loss: 0.334231] [G loss: 0.266705] [info loss: 1.536522]\n",
            "[Epoch 8/200] [Batch 8/18] [D loss: 0.258530] [G loss: 0.397784] [info loss: 1.551753]\n",
            "[Epoch 8/200] [Batch 9/18] [D loss: 0.310577] [G loss: 0.337233] [info loss: 1.537647]\n",
            "[Epoch 8/200] [Batch 10/18] [D loss: 0.305642] [G loss: 0.371981] [info loss: 1.527635]\n",
            "[Epoch 8/200] [Batch 11/18] [D loss: 0.308847] [G loss: 0.327162] [info loss: 1.536590]\n",
            "[Epoch 8/200] [Batch 12/18] [D loss: 0.306390] [G loss: 0.383452] [info loss: 1.548513]\n",
            "[Epoch 8/200] [Batch 13/18] [D loss: 0.297045] [G loss: 0.368282] [info loss: 1.522665]\n",
            "[Epoch 8/200] [Batch 14/18] [D loss: 0.284920] [G loss: 0.455857] [info loss: 1.527047]\n",
            "[Epoch 8/200] [Batch 15/18] [D loss: 0.298284] [G loss: 0.380417] [info loss: 1.530290]\n",
            "[Epoch 8/200] [Batch 16/18] [D loss: 0.269405] [G loss: 0.394947] [info loss: 1.523776]\n",
            "[Epoch 8/200] [Batch 17/18] [D loss: 0.251435] [G loss: 0.356455] [info loss: 1.561186]\n",
            "[Epoch 9/200] [Batch 0/18] [D loss: 0.325855] [G loss: 0.304460] [info loss: 1.519821]\n",
            "[Epoch 9/200] [Batch 1/18] [D loss: 0.286582] [G loss: 0.463327] [info loss: 1.533798]\n",
            "[Epoch 9/200] [Batch 2/18] [D loss: 0.242836] [G loss: 0.391534] [info loss: 1.529138]\n",
            "[Epoch 9/200] [Batch 3/18] [D loss: 0.330115] [G loss: 0.371534] [info loss: 1.553054]\n",
            "[Epoch 9/200] [Batch 4/18] [D loss: 0.274177] [G loss: 0.425651] [info loss: 1.523058]\n",
            "[Epoch 9/200] [Batch 5/18] [D loss: 0.292085] [G loss: 0.353896] [info loss: 1.531624]\n",
            "[Epoch 9/200] [Batch 6/18] [D loss: 0.275263] [G loss: 0.354600] [info loss: 1.525685]\n",
            "[Epoch 9/200] [Batch 7/18] [D loss: 0.296925] [G loss: 0.355984] [info loss: 1.525885]\n",
            "[Epoch 9/200] [Batch 8/18] [D loss: 0.297062] [G loss: 0.331062] [info loss: 1.525695]\n",
            "[Epoch 9/200] [Batch 9/18] [D loss: 0.290499] [G loss: 0.377658] [info loss: 1.537569]\n",
            "[Epoch 9/200] [Batch 10/18] [D loss: 0.319800] [G loss: 0.406646] [info loss: 1.529256]\n",
            "[Epoch 9/200] [Batch 11/18] [D loss: 0.290848] [G loss: 0.430955] [info loss: 1.514076]\n",
            "[Epoch 9/200] [Batch 12/18] [D loss: 0.235179] [G loss: 0.352586] [info loss: 1.527061]\n",
            "[Epoch 9/200] [Batch 13/18] [D loss: 0.271674] [G loss: 0.400146] [info loss: 1.520956]\n",
            "[Epoch 9/200] [Batch 14/18] [D loss: 0.271358] [G loss: 0.327668] [info loss: 1.521795]\n",
            "[Epoch 9/200] [Batch 15/18] [D loss: 0.320079] [G loss: 0.326959] [info loss: 1.512949]\n",
            "[Epoch 9/200] [Batch 16/18] [D loss: 0.259396] [G loss: 0.353468] [info loss: 1.526910]\n",
            "[Epoch 9/200] [Batch 17/18] [D loss: 0.277194] [G loss: 0.321155] [info loss: 1.550630]\n",
            "[Epoch 10/200] [Batch 0/18] [D loss: 0.260209] [G loss: 0.401365] [info loss: 1.523281]\n",
            "[Epoch 10/200] [Batch 1/18] [D loss: 0.292550] [G loss: 0.301187] [info loss: 1.521593]\n",
            "[Epoch 10/200] [Batch 2/18] [D loss: 0.264027] [G loss: 0.304796] [info loss: 1.511245]\n",
            "[Epoch 10/200] [Batch 3/18] [D loss: 0.255253] [G loss: 0.347610] [info loss: 1.508369]\n",
            "[Epoch 10/200] [Batch 4/18] [D loss: 0.303438] [G loss: 0.333139] [info loss: 1.511574]\n",
            "[Epoch 10/200] [Batch 5/18] [D loss: 0.271127] [G loss: 0.348212] [info loss: 1.536148]\n",
            "[Epoch 10/200] [Batch 6/18] [D loss: 0.300897] [G loss: 0.381444] [info loss: 1.519275]\n",
            "[Epoch 10/200] [Batch 7/18] [D loss: 0.303371] [G loss: 0.338097] [info loss: 1.514722]\n",
            "[Epoch 10/200] [Batch 8/18] [D loss: 0.257203] [G loss: 0.303318] [info loss: 1.512835]\n",
            "[Epoch 10/200] [Batch 9/18] [D loss: 0.287538] [G loss: 0.326526] [info loss: 1.515345]\n",
            "[Epoch 10/200] [Batch 10/18] [D loss: 0.309412] [G loss: 0.380557] [info loss: 1.516569]\n",
            "[Epoch 10/200] [Batch 11/18] [D loss: 0.296357] [G loss: 0.387678] [info loss: 1.525860]\n",
            "[Epoch 10/200] [Batch 12/18] [D loss: 0.245928] [G loss: 0.319623] [info loss: 1.519739]\n",
            "[Epoch 10/200] [Batch 13/18] [D loss: 0.274043] [G loss: 0.373174] [info loss: 1.519848]\n",
            "[Epoch 10/200] [Batch 14/18] [D loss: 0.335908] [G loss: 0.304330] [info loss: 1.513981]\n",
            "[Epoch 10/200] [Batch 15/18] [D loss: 0.275811] [G loss: 0.301755] [info loss: 1.519625]\n",
            "[Epoch 10/200] [Batch 16/18] [D loss: 0.259680] [G loss: 0.401103] [info loss: 1.510155]\n",
            "[Epoch 10/200] [Batch 17/18] [D loss: 0.279760] [G loss: 0.406594] [info loss: 1.515927]\n",
            "[Epoch 11/200] [Batch 0/18] [D loss: 0.245812] [G loss: 0.363120] [info loss: 1.518097]\n",
            "[Epoch 11/200] [Batch 1/18] [D loss: 0.273704] [G loss: 0.333628] [info loss: 1.523233]\n",
            "[Epoch 11/200] [Batch 2/18] [D loss: 0.272050] [G loss: 0.399376] [info loss: 1.511616]\n",
            "[Epoch 11/200] [Batch 3/18] [D loss: 0.289283] [G loss: 0.348269] [info loss: 1.513117]\n",
            "[Epoch 11/200] [Batch 4/18] [D loss: 0.272262] [G loss: 0.407883] [info loss: 1.518557]\n",
            "[Epoch 11/200] [Batch 5/18] [D loss: 0.274930] [G loss: 0.438640] [info loss: 1.503471]\n",
            "[Epoch 11/200] [Batch 6/18] [D loss: 0.250798] [G loss: 0.318229] [info loss: 1.505507]\n",
            "[Epoch 11/200] [Batch 7/18] [D loss: 0.269425] [G loss: 0.392058] [info loss: 1.518972]\n",
            "[Epoch 11/200] [Batch 8/18] [D loss: 0.255647] [G loss: 0.348011] [info loss: 1.504148]\n",
            "[Epoch 11/200] [Batch 9/18] [D loss: 0.288326] [G loss: 0.406860] [info loss: 1.516114]\n",
            "[Epoch 11/200] [Batch 10/18] [D loss: 0.260174] [G loss: 0.326406] [info loss: 1.535593]\n",
            "[Epoch 11/200] [Batch 11/18] [D loss: 0.236437] [G loss: 0.364637] [info loss: 1.500190]\n",
            "[Epoch 11/200] [Batch 12/18] [D loss: 0.262312] [G loss: 0.336559] [info loss: 1.509638]\n",
            "[Epoch 11/200] [Batch 13/18] [D loss: 0.273058] [G loss: 0.358342] [info loss: 1.507149]\n",
            "[Epoch 11/200] [Batch 14/18] [D loss: 0.262743] [G loss: 0.308803] [info loss: 1.519865]\n",
            "[Epoch 11/200] [Batch 15/18] [D loss: 0.299904] [G loss: 0.360664] [info loss: 1.507834]\n",
            "[Epoch 11/200] [Batch 16/18] [D loss: 0.267399] [G loss: 0.394401] [info loss: 1.503385]\n",
            "[Epoch 11/200] [Batch 17/18] [D loss: 0.285601] [G loss: 0.290396] [info loss: 1.513168]\n",
            "[Epoch 12/200] [Batch 0/18] [D loss: 0.284846] [G loss: 0.360024] [info loss: 1.520525]\n",
            "[Epoch 12/200] [Batch 1/18] [D loss: 0.291989] [G loss: 0.345526] [info loss: 1.509893]\n",
            "[Epoch 12/200] [Batch 2/18] [D loss: 0.259458] [G loss: 0.363882] [info loss: 1.520146]\n",
            "[Epoch 12/200] [Batch 3/18] [D loss: 0.287700] [G loss: 0.357499] [info loss: 1.502660]\n",
            "[Epoch 12/200] [Batch 4/18] [D loss: 0.265924] [G loss: 0.340061] [info loss: 1.511667]\n",
            "[Epoch 12/200] [Batch 5/18] [D loss: 0.261229] [G loss: 0.315527] [info loss: 1.510593]\n",
            "[Epoch 12/200] [Batch 6/18] [D loss: 0.242341] [G loss: 0.349666] [info loss: 1.515630]\n",
            "[Epoch 12/200] [Batch 7/18] [D loss: 0.282227] [G loss: 0.339624] [info loss: 1.506559]\n",
            "[Epoch 12/200] [Batch 8/18] [D loss: 0.269736] [G loss: 0.342646] [info loss: 1.514077]\n",
            "[Epoch 12/200] [Batch 9/18] [D loss: 0.294720] [G loss: 0.279775] [info loss: 1.506336]\n",
            "[Epoch 12/200] [Batch 10/18] [D loss: 0.288854] [G loss: 0.370954] [info loss: 1.511716]\n",
            "[Epoch 12/200] [Batch 11/18] [D loss: 0.287695] [G loss: 0.349104] [info loss: 1.497026]\n",
            "[Epoch 12/200] [Batch 12/18] [D loss: 0.271226] [G loss: 0.348435] [info loss: 1.508988]\n",
            "[Epoch 12/200] [Batch 13/18] [D loss: 0.298875] [G loss: 0.396065] [info loss: 1.501760]\n",
            "[Epoch 12/200] [Batch 14/18] [D loss: 0.258780] [G loss: 0.383158] [info loss: 1.498713]\n",
            "[Epoch 12/200] [Batch 15/18] [D loss: 0.258731] [G loss: 0.325558] [info loss: 1.501945]\n",
            "[Epoch 12/200] [Batch 16/18] [D loss: 0.261986] [G loss: 0.406584] [info loss: 1.505388]\n",
            "[Epoch 12/200] [Batch 17/18] [D loss: 0.307325] [G loss: 0.413404] [info loss: 1.514483]\n",
            "[Epoch 13/200] [Batch 0/18] [D loss: 0.269972] [G loss: 0.268772] [info loss: 1.519029]\n",
            "[Epoch 13/200] [Batch 1/18] [D loss: 0.256725] [G loss: 0.363813] [info loss: 1.505696]\n",
            "[Epoch 13/200] [Batch 2/18] [D loss: 0.279023] [G loss: 0.320974] [info loss: 1.511812]\n",
            "[Epoch 13/200] [Batch 3/18] [D loss: 0.267237] [G loss: 0.329425] [info loss: 1.504746]\n",
            "[Epoch 13/200] [Batch 4/18] [D loss: 0.277712] [G loss: 0.350639] [info loss: 1.513366]\n",
            "[Epoch 13/200] [Batch 5/18] [D loss: 0.274185] [G loss: 0.339876] [info loss: 1.524893]\n",
            "[Epoch 13/200] [Batch 6/18] [D loss: 0.261359] [G loss: 0.354217] [info loss: 1.519032]\n",
            "[Epoch 13/200] [Batch 7/18] [D loss: 0.267090] [G loss: 0.325177] [info loss: 1.508435]\n",
            "[Epoch 13/200] [Batch 8/18] [D loss: 0.233332] [G loss: 0.338534] [info loss: 1.503615]\n",
            "[Epoch 13/200] [Batch 9/18] [D loss: 0.270735] [G loss: 0.352671] [info loss: 1.503421]\n",
            "[Epoch 13/200] [Batch 10/18] [D loss: 0.267924] [G loss: 0.338241] [info loss: 1.517371]\n",
            "[Epoch 13/200] [Batch 11/18] [D loss: 0.237832] [G loss: 0.436110] [info loss: 1.489504]\n",
            "[Epoch 13/200] [Batch 12/18] [D loss: 0.276448] [G loss: 0.332778] [info loss: 1.497146]\n",
            "[Epoch 13/200] [Batch 13/18] [D loss: 0.280850] [G loss: 0.350446] [info loss: 1.511502]\n",
            "[Epoch 13/200] [Batch 14/18] [D loss: 0.263828] [G loss: 0.345162] [info loss: 1.500542]\n",
            "[Epoch 13/200] [Batch 15/18] [D loss: 0.285199] [G loss: 0.294851] [info loss: 1.497243]\n",
            "[Epoch 13/200] [Batch 16/18] [D loss: 0.278099] [G loss: 0.336579] [info loss: 1.510362]\n",
            "[Epoch 13/200] [Batch 17/18] [D loss: 0.276283] [G loss: 0.447257] [info loss: 1.494637]\n",
            "[Epoch 14/200] [Batch 0/18] [D loss: 0.270893] [G loss: 0.340179] [info loss: 1.500055]\n",
            "[Epoch 14/200] [Batch 1/18] [D loss: 0.270537] [G loss: 0.367536] [info loss: 1.499436]\n",
            "[Epoch 14/200] [Batch 2/18] [D loss: 0.278514] [G loss: 0.342559] [info loss: 1.492437]\n",
            "[Epoch 14/200] [Batch 3/18] [D loss: 0.264402] [G loss: 0.380785] [info loss: 1.500812]\n",
            "[Epoch 14/200] [Batch 4/18] [D loss: 0.303947] [G loss: 0.316162] [info loss: 1.509908]\n",
            "[Epoch 14/200] [Batch 5/18] [D loss: 0.274792] [G loss: 0.326055] [info loss: 1.502489]\n",
            "[Epoch 14/200] [Batch 6/18] [D loss: 0.248173] [G loss: 0.455017] [info loss: 1.501079]\n",
            "[Epoch 14/200] [Batch 7/18] [D loss: 0.247931] [G loss: 0.336249] [info loss: 1.503564]\n",
            "[Epoch 14/200] [Batch 8/18] [D loss: 0.288216] [G loss: 0.443657] [info loss: 1.498112]\n",
            "[Epoch 14/200] [Batch 9/18] [D loss: 0.279419] [G loss: 0.302520] [info loss: 1.513349]\n",
            "[Epoch 14/200] [Batch 10/18] [D loss: 0.270984] [G loss: 0.293256] [info loss: 1.510360]\n",
            "[Epoch 14/200] [Batch 11/18] [D loss: 0.292009] [G loss: 0.312847] [info loss: 1.501661]\n",
            "[Epoch 14/200] [Batch 12/18] [D loss: 0.226269] [G loss: 0.360120] [info loss: 1.501497]\n",
            "[Epoch 14/200] [Batch 13/18] [D loss: 0.283830] [G loss: 0.345154] [info loss: 1.515382]\n",
            "[Epoch 14/200] [Batch 14/18] [D loss: 0.254955] [G loss: 0.381233] [info loss: 1.507033]\n",
            "[Epoch 14/200] [Batch 15/18] [D loss: 0.286102] [G loss: 0.337900] [info loss: 1.511542]\n",
            "[Epoch 14/200] [Batch 16/18] [D loss: 0.262731] [G loss: 0.366367] [info loss: 1.502563]\n",
            "[Epoch 14/200] [Batch 17/18] [D loss: 0.262135] [G loss: 0.449225] [info loss: 1.502896]\n",
            "[Epoch 15/200] [Batch 0/18] [D loss: 0.261838] [G loss: 0.439934] [info loss: 1.498199]\n",
            "[Epoch 15/200] [Batch 1/18] [D loss: 0.259601] [G loss: 0.384288] [info loss: 1.510207]\n",
            "[Epoch 15/200] [Batch 2/18] [D loss: 0.271422] [G loss: 0.334546] [info loss: 1.502562]\n",
            "[Epoch 15/200] [Batch 3/18] [D loss: 0.262322] [G loss: 0.355580] [info loss: 1.505851]\n",
            "[Epoch 15/200] [Batch 4/18] [D loss: 0.281667] [G loss: 0.391541] [info loss: 1.503664]\n",
            "[Epoch 15/200] [Batch 5/18] [D loss: 0.239230] [G loss: 0.383720] [info loss: 1.495805]\n",
            "[Epoch 15/200] [Batch 6/18] [D loss: 0.226765] [G loss: 0.369267] [info loss: 1.506210]\n",
            "[Epoch 15/200] [Batch 7/18] [D loss: 0.270457] [G loss: 0.372755] [info loss: 1.501499]\n",
            "[Epoch 15/200] [Batch 8/18] [D loss: 0.301693] [G loss: 0.326986] [info loss: 1.498139]\n",
            "[Epoch 15/200] [Batch 9/18] [D loss: 0.329897] [G loss: 0.305045] [info loss: 1.496983]\n",
            "[Epoch 15/200] [Batch 10/18] [D loss: 0.236309] [G loss: 0.391822] [info loss: 1.490890]\n",
            "[Epoch 15/200] [Batch 11/18] [D loss: 0.266768] [G loss: 0.364944] [info loss: 1.492380]\n",
            "[Epoch 15/200] [Batch 12/18] [D loss: 0.262489] [G loss: 0.342506] [info loss: 1.499158]\n",
            "[Epoch 15/200] [Batch 13/18] [D loss: 0.245478] [G loss: 0.359934] [info loss: 1.486711]\n",
            "[Epoch 15/200] [Batch 14/18] [D loss: 0.281657] [G loss: 0.371463] [info loss: 1.497097]\n",
            "[Epoch 15/200] [Batch 15/18] [D loss: 0.297045] [G loss: 0.259087] [info loss: 1.513306]\n",
            "[Epoch 15/200] [Batch 16/18] [D loss: 0.242936] [G loss: 0.320183] [info loss: 1.498036]\n",
            "[Epoch 15/200] [Batch 17/18] [D loss: 0.275700] [G loss: 0.397151] [info loss: 1.508696]\n",
            "[Epoch 16/200] [Batch 0/18] [D loss: 0.243590] [G loss: 0.374458] [info loss: 1.496334]\n",
            "[Epoch 16/200] [Batch 1/18] [D loss: 0.263638] [G loss: 0.283143] [info loss: 1.501995]\n",
            "[Epoch 16/200] [Batch 2/18] [D loss: 0.268564] [G loss: 0.290402] [info loss: 1.500833]\n",
            "[Epoch 16/200] [Batch 3/18] [D loss: 0.282185] [G loss: 0.337592] [info loss: 1.492324]\n",
            "[Epoch 16/200] [Batch 4/18] [D loss: 0.263164] [G loss: 0.337289] [info loss: 1.496375]\n",
            "[Epoch 16/200] [Batch 5/18] [D loss: 0.271020] [G loss: 0.345315] [info loss: 1.506284]\n",
            "[Epoch 16/200] [Batch 6/18] [D loss: 0.267626] [G loss: 0.391622] [info loss: 1.499839]\n",
            "[Epoch 16/200] [Batch 7/18] [D loss: 0.247010] [G loss: 0.340098] [info loss: 1.504521]\n",
            "[Epoch 16/200] [Batch 8/18] [D loss: 0.257633] [G loss: 0.380531] [info loss: 1.491772]\n",
            "[Epoch 16/200] [Batch 9/18] [D loss: 0.262692] [G loss: 0.309155] [info loss: 1.514792]\n",
            "[Epoch 16/200] [Batch 10/18] [D loss: 0.259467] [G loss: 0.370338] [info loss: 1.497001]\n",
            "[Epoch 16/200] [Batch 11/18] [D loss: 0.285056] [G loss: 0.285768] [info loss: 1.510950]\n",
            "[Epoch 16/200] [Batch 12/18] [D loss: 0.288670] [G loss: 0.365299] [info loss: 1.503533]\n",
            "[Epoch 16/200] [Batch 13/18] [D loss: 0.236351] [G loss: 0.489245] [info loss: 1.493996]\n",
            "[Epoch 16/200] [Batch 14/18] [D loss: 0.267026] [G loss: 0.393114] [info loss: 1.498553]\n",
            "[Epoch 16/200] [Batch 15/18] [D loss: 0.257892] [G loss: 0.349903] [info loss: 1.496830]\n",
            "[Epoch 16/200] [Batch 16/18] [D loss: 0.277187] [G loss: 0.323154] [info loss: 1.502762]\n",
            "[Epoch 16/200] [Batch 17/18] [D loss: 0.252584] [G loss: 0.328191] [info loss: 1.505442]\n",
            "[Epoch 17/200] [Batch 0/18] [D loss: 0.217622] [G loss: 0.356012] [info loss: 1.499074]\n",
            "[Epoch 17/200] [Batch 1/18] [D loss: 0.301029] [G loss: 0.261405] [info loss: 1.491953]\n",
            "[Epoch 17/200] [Batch 2/18] [D loss: 0.256576] [G loss: 0.310130] [info loss: 1.489378]\n",
            "[Epoch 17/200] [Batch 3/18] [D loss: 0.267008] [G loss: 0.312344] [info loss: 1.525120]\n",
            "[Epoch 17/200] [Batch 4/18] [D loss: 0.257811] [G loss: 0.375260] [info loss: 1.493132]\n",
            "[Epoch 17/200] [Batch 5/18] [D loss: 0.254853] [G loss: 0.352369] [info loss: 1.494646]\n",
            "[Epoch 17/200] [Batch 6/18] [D loss: 0.261818] [G loss: 0.327249] [info loss: 1.507140]\n",
            "[Epoch 17/200] [Batch 7/18] [D loss: 0.243388] [G loss: 0.351841] [info loss: 1.503360]\n",
            "[Epoch 17/200] [Batch 8/18] [D loss: 0.275186] [G loss: 0.292415] [info loss: 1.492532]\n",
            "[Epoch 17/200] [Batch 9/18] [D loss: 0.266767] [G loss: 0.335741] [info loss: 1.486909]\n",
            "[Epoch 17/200] [Batch 10/18] [D loss: 0.263018] [G loss: 0.358143] [info loss: 1.501333]\n",
            "[Epoch 17/200] [Batch 11/18] [D loss: 0.241003] [G loss: 0.291178] [info loss: 1.512340]\n",
            "[Epoch 17/200] [Batch 12/18] [D loss: 0.268777] [G loss: 0.304600] [info loss: 1.496047]\n",
            "[Epoch 17/200] [Batch 13/18] [D loss: 0.262520] [G loss: 0.330929] [info loss: 1.491638]\n",
            "[Epoch 17/200] [Batch 14/18] [D loss: 0.257689] [G loss: 0.391574] [info loss: 1.494002]\n",
            "[Epoch 17/200] [Batch 15/18] [D loss: 0.209255] [G loss: 0.381058] [info loss: 1.497351]\n",
            "[Epoch 17/200] [Batch 16/18] [D loss: 0.278191] [G loss: 0.344090] [info loss: 1.487263]\n",
            "[Epoch 17/200] [Batch 17/18] [D loss: 0.267597] [G loss: 0.388938] [info loss: 1.484613]\n",
            "[Epoch 18/200] [Batch 0/18] [D loss: 0.279315] [G loss: 0.339800] [info loss: 1.489804]\n",
            "[Epoch 18/200] [Batch 1/18] [D loss: 0.281941] [G loss: 0.339465] [info loss: 1.489714]\n",
            "[Epoch 18/200] [Batch 2/18] [D loss: 0.258095] [G loss: 0.334122] [info loss: 1.500943]\n",
            "[Epoch 18/200] [Batch 3/18] [D loss: 0.261690] [G loss: 0.380753] [info loss: 1.511740]\n",
            "[Epoch 18/200] [Batch 4/18] [D loss: 0.292850] [G loss: 0.306937] [info loss: 1.507328]\n",
            "[Epoch 18/200] [Batch 5/18] [D loss: 0.245180] [G loss: 0.330856] [info loss: 1.515131]\n",
            "[Epoch 18/200] [Batch 6/18] [D loss: 0.241916] [G loss: 0.381151] [info loss: 1.492985]\n",
            "[Epoch 18/200] [Batch 7/18] [D loss: 0.287523] [G loss: 0.361573] [info loss: 1.485936]\n",
            "[Epoch 18/200] [Batch 8/18] [D loss: 0.264028] [G loss: 0.333670] [info loss: 1.494628]\n",
            "[Epoch 18/200] [Batch 9/18] [D loss: 0.253419] [G loss: 0.365973] [info loss: 1.505833]\n",
            "[Epoch 18/200] [Batch 10/18] [D loss: 0.279062] [G loss: 0.411444] [info loss: 1.491248]\n",
            "[Epoch 18/200] [Batch 11/18] [D loss: 0.284953] [G loss: 0.334079] [info loss: 1.493853]\n",
            "[Epoch 18/200] [Batch 12/18] [D loss: 0.235861] [G loss: 0.379796] [info loss: 1.499488]\n",
            "[Epoch 18/200] [Batch 13/18] [D loss: 0.247410] [G loss: 0.389057] [info loss: 1.492135]\n",
            "[Epoch 18/200] [Batch 14/18] [D loss: 0.258376] [G loss: 0.331132] [info loss: 1.483936]\n",
            "[Epoch 18/200] [Batch 15/18] [D loss: 0.275054] [G loss: 0.327734] [info loss: 1.498003]\n",
            "[Epoch 18/200] [Batch 16/18] [D loss: 0.274588] [G loss: 0.343615] [info loss: 1.498788]\n",
            "[Epoch 18/200] [Batch 17/18] [D loss: 0.223390] [G loss: 0.425650] [info loss: 1.497907]\n",
            "[Epoch 19/200] [Batch 0/18] [D loss: 0.261624] [G loss: 0.358961] [info loss: 1.504622]\n",
            "[Epoch 19/200] [Batch 1/18] [D loss: 0.259238] [G loss: 0.340054] [info loss: 1.484158]\n",
            "[Epoch 19/200] [Batch 2/18] [D loss: 0.288273] [G loss: 0.316264] [info loss: 1.486797]\n",
            "[Epoch 19/200] [Batch 3/18] [D loss: 0.268600] [G loss: 0.308139] [info loss: 1.505098]\n",
            "[Epoch 19/200] [Batch 4/18] [D loss: 0.269528] [G loss: 0.369343] [info loss: 1.501268]\n",
            "[Epoch 19/200] [Batch 5/18] [D loss: 0.244458] [G loss: 0.370007] [info loss: 1.493769]\n",
            "[Epoch 19/200] [Batch 6/18] [D loss: 0.242273] [G loss: 0.368061] [info loss: 1.487268]\n",
            "[Epoch 19/200] [Batch 7/18] [D loss: 0.244546] [G loss: 0.352107] [info loss: 1.497712]\n",
            "[Epoch 19/200] [Batch 8/18] [D loss: 0.217926] [G loss: 0.350559] [info loss: 1.488298]\n",
            "[Epoch 19/200] [Batch 9/18] [D loss: 0.274008] [G loss: 0.333880] [info loss: 1.492992]\n",
            "[Epoch 19/200] [Batch 10/18] [D loss: 0.269037] [G loss: 0.367240] [info loss: 1.502237]\n",
            "[Epoch 19/200] [Batch 11/18] [D loss: 0.259408] [G loss: 0.366082] [info loss: 1.487088]\n",
            "[Epoch 19/200] [Batch 12/18] [D loss: 0.269091] [G loss: 0.326729] [info loss: 1.486794]\n",
            "[Epoch 19/200] [Batch 13/18] [D loss: 0.235660] [G loss: 0.351605] [info loss: 1.493856]\n",
            "[Epoch 19/200] [Batch 14/18] [D loss: 0.246118] [G loss: 0.325648] [info loss: 1.482672]\n",
            "[Epoch 19/200] [Batch 15/18] [D loss: 0.247628] [G loss: 0.373081] [info loss: 1.479877]\n",
            "[Epoch 19/200] [Batch 16/18] [D loss: 0.249119] [G loss: 0.328897] [info loss: 1.489992]\n",
            "[Epoch 19/200] [Batch 17/18] [D loss: 0.275279] [G loss: 0.323502] [info loss: 1.493699]\n",
            "[Epoch 20/200] [Batch 0/18] [D loss: 0.272849] [G loss: 0.316678] [info loss: 1.497898]\n",
            "[Epoch 20/200] [Batch 1/18] [D loss: 0.217207] [G loss: 0.359045] [info loss: 1.486488]\n",
            "[Epoch 20/200] [Batch 2/18] [D loss: 0.253919] [G loss: 0.358362] [info loss: 1.490845]\n",
            "[Epoch 20/200] [Batch 3/18] [D loss: 0.287515] [G loss: 0.356271] [info loss: 1.493335]\n",
            "[Epoch 20/200] [Batch 4/18] [D loss: 0.257234] [G loss: 0.400734] [info loss: 1.495216]\n",
            "[Epoch 20/200] [Batch 5/18] [D loss: 0.233819] [G loss: 0.324914] [info loss: 1.506090]\n",
            "[Epoch 20/200] [Batch 6/18] [D loss: 0.241048] [G loss: 0.342672] [info loss: 1.504611]\n",
            "[Epoch 20/200] [Batch 7/18] [D loss: 0.242747] [G loss: 0.336504] [info loss: 1.527493]\n",
            "[Epoch 20/200] [Batch 8/18] [D loss: 0.260074] [G loss: 0.282496] [info loss: 1.530468]\n",
            "[Epoch 20/200] [Batch 9/18] [D loss: 0.248352] [G loss: 0.349397] [info loss: 1.491197]\n",
            "[Epoch 20/200] [Batch 10/18] [D loss: 0.226441] [G loss: 0.328208] [info loss: 1.486146]\n",
            "[Epoch 20/200] [Batch 11/18] [D loss: 0.257936] [G loss: 0.339703] [info loss: 1.493987]\n",
            "[Epoch 20/200] [Batch 12/18] [D loss: 0.245664] [G loss: 0.337471] [info loss: 1.494354]\n",
            "[Epoch 20/200] [Batch 13/18] [D loss: 0.235200] [G loss: 0.330448] [info loss: 1.487487]\n",
            "[Epoch 20/200] [Batch 14/18] [D loss: 0.244733] [G loss: 0.388055] [info loss: 1.490214]\n",
            "[Epoch 20/200] [Batch 15/18] [D loss: 0.226840] [G loss: 0.388157] [info loss: 1.479085]\n",
            "[Epoch 20/200] [Batch 16/18] [D loss: 0.262729] [G loss: 0.341303] [info loss: 1.485740]\n",
            "[Epoch 20/200] [Batch 17/18] [D loss: 0.237092] [G loss: 0.333405] [info loss: 1.495870]\n",
            "[Epoch 21/200] [Batch 0/18] [D loss: 0.257510] [G loss: 0.333587] [info loss: 1.491961]\n",
            "[Epoch 21/200] [Batch 1/18] [D loss: 0.240930] [G loss: 0.335331] [info loss: 1.487808]\n",
            "[Epoch 21/200] [Batch 2/18] [D loss: 0.266562] [G loss: 0.367548] [info loss: 1.491996]\n",
            "[Epoch 21/200] [Batch 3/18] [D loss: 0.221800] [G loss: 0.340803] [info loss: 1.484079]\n",
            "[Epoch 21/200] [Batch 4/18] [D loss: 0.245559] [G loss: 0.356938] [info loss: 1.483563]\n",
            "[Epoch 21/200] [Batch 5/18] [D loss: 0.247914] [G loss: 0.330106] [info loss: 1.490303]\n",
            "[Epoch 21/200] [Batch 6/18] [D loss: 0.285713] [G loss: 0.294434] [info loss: 1.490699]\n",
            "[Epoch 21/200] [Batch 7/18] [D loss: 0.242365] [G loss: 0.331093] [info loss: 1.509286]\n",
            "[Epoch 21/200] [Batch 8/18] [D loss: 0.271316] [G loss: 0.307647] [info loss: 1.500437]\n",
            "[Epoch 21/200] [Batch 9/18] [D loss: 0.248544] [G loss: 0.322983] [info loss: 1.490349]\n",
            "[Epoch 21/200] [Batch 10/18] [D loss: 0.251096] [G loss: 0.359623] [info loss: 1.498982]\n",
            "[Epoch 21/200] [Batch 11/18] [D loss: 0.261395] [G loss: 0.372188] [info loss: 1.495683]\n",
            "[Epoch 21/200] [Batch 12/18] [D loss: 0.263543] [G loss: 0.348138] [info loss: 1.479638]\n",
            "[Epoch 21/200] [Batch 13/18] [D loss: 0.266474] [G loss: 0.307713] [info loss: 1.501864]\n",
            "[Epoch 21/200] [Batch 14/18] [D loss: 0.248246] [G loss: 0.333077] [info loss: 1.495836]\n",
            "[Epoch 21/200] [Batch 15/18] [D loss: 0.269588] [G loss: 0.321389] [info loss: 1.493636]\n",
            "[Epoch 21/200] [Batch 16/18] [D loss: 0.240738] [G loss: 0.361988] [info loss: 1.495088]\n",
            "[Epoch 21/200] [Batch 17/18] [D loss: 0.256335] [G loss: 0.326859] [info loss: 1.502028]\n",
            "[Epoch 22/200] [Batch 0/18] [D loss: 0.246819] [G loss: 0.405213] [info loss: 1.494412]\n",
            "[Epoch 22/200] [Batch 1/18] [D loss: 0.275361] [G loss: 0.328305] [info loss: 1.500823]\n",
            "[Epoch 22/200] [Batch 2/18] [D loss: 0.260350] [G loss: 0.353702] [info loss: 1.487385]\n",
            "[Epoch 22/200] [Batch 3/18] [D loss: 0.263723] [G loss: 0.297831] [info loss: 1.488580]\n",
            "[Epoch 22/200] [Batch 4/18] [D loss: 0.246806] [G loss: 0.350230] [info loss: 1.486632]\n",
            "[Epoch 22/200] [Batch 5/18] [D loss: 0.236773] [G loss: 0.326819] [info loss: 1.487830]\n",
            "[Epoch 22/200] [Batch 6/18] [D loss: 0.241950] [G loss: 0.285821] [info loss: 1.497140]\n",
            "[Epoch 22/200] [Batch 7/18] [D loss: 0.286913] [G loss: 0.414434] [info loss: 1.481562]\n",
            "[Epoch 22/200] [Batch 8/18] [D loss: 0.232828] [G loss: 0.345402] [info loss: 1.489442]\n",
            "[Epoch 22/200] [Batch 9/18] [D loss: 0.246477] [G loss: 0.311500] [info loss: 1.481844]\n",
            "[Epoch 22/200] [Batch 10/18] [D loss: 0.265783] [G loss: 0.332337] [info loss: 1.490139]\n",
            "[Epoch 22/200] [Batch 11/18] [D loss: 0.291766] [G loss: 0.339781] [info loss: 1.499060]\n",
            "[Epoch 22/200] [Batch 12/18] [D loss: 0.240327] [G loss: 0.330793] [info loss: 1.492244]\n",
            "[Epoch 22/200] [Batch 13/18] [D loss: 0.256131] [G loss: 0.329996] [info loss: 1.498364]\n",
            "[Epoch 22/200] [Batch 14/18] [D loss: 0.242295] [G loss: 0.350881] [info loss: 1.504090]\n",
            "[Epoch 22/200] [Batch 15/18] [D loss: 0.270108] [G loss: 0.397912] [info loss: 1.485105]\n",
            "[Epoch 22/200] [Batch 16/18] [D loss: 0.248149] [G loss: 0.357411] [info loss: 1.484773]\n",
            "[Epoch 22/200] [Batch 17/18] [D loss: 0.263389] [G loss: 0.360514] [info loss: 1.497067]\n",
            "[Epoch 23/200] [Batch 0/18] [D loss: 0.253492] [G loss: 0.354748] [info loss: 1.509044]\n",
            "[Epoch 23/200] [Batch 1/18] [D loss: 0.254147] [G loss: 0.317327] [info loss: 1.485371]\n",
            "[Epoch 23/200] [Batch 2/18] [D loss: 0.252247] [G loss: 0.347355] [info loss: 1.487432]\n",
            "[Epoch 23/200] [Batch 3/18] [D loss: 0.254469] [G loss: 0.337082] [info loss: 1.497067]\n",
            "[Epoch 23/200] [Batch 4/18] [D loss: 0.275437] [G loss: 0.284780] [info loss: 1.501078]\n",
            "[Epoch 23/200] [Batch 5/18] [D loss: 0.292166] [G loss: 0.337278] [info loss: 1.497163]\n",
            "[Epoch 23/200] [Batch 6/18] [D loss: 0.289689] [G loss: 0.365732] [info loss: 1.491971]\n",
            "[Epoch 23/200] [Batch 7/18] [D loss: 0.274979] [G loss: 0.373403] [info loss: 1.505572]\n",
            "[Epoch 23/200] [Batch 8/18] [D loss: 0.246777] [G loss: 0.379629] [info loss: 1.500033]\n",
            "[Epoch 23/200] [Batch 9/18] [D loss: 0.283218] [G loss: 0.304119] [info loss: 1.485673]\n",
            "[Epoch 23/200] [Batch 10/18] [D loss: 0.251827] [G loss: 0.394312] [info loss: 1.482880]\n",
            "[Epoch 23/200] [Batch 11/18] [D loss: 0.223993] [G loss: 0.335868] [info loss: 1.484487]\n",
            "[Epoch 23/200] [Batch 12/18] [D loss: 0.254658] [G loss: 0.382022] [info loss: 1.493165]\n",
            "[Epoch 23/200] [Batch 13/18] [D loss: 0.256142] [G loss: 0.395880] [info loss: 1.488211]\n",
            "[Epoch 23/200] [Batch 14/18] [D loss: 0.261096] [G loss: 0.351491] [info loss: 1.500449]\n",
            "[Epoch 23/200] [Batch 15/18] [D loss: 0.237844] [G loss: 0.339137] [info loss: 1.488150]\n",
            "[Epoch 23/200] [Batch 16/18] [D loss: 0.244808] [G loss: 0.308608] [info loss: 1.480460]\n",
            "[Epoch 23/200] [Batch 17/18] [D loss: 0.280362] [G loss: 0.341318] [info loss: 1.494946]\n",
            "[Epoch 24/200] [Batch 0/18] [D loss: 0.271144] [G loss: 0.325244] [info loss: 1.484733]\n",
            "[Epoch 24/200] [Batch 1/18] [D loss: 0.275849] [G loss: 0.344754] [info loss: 1.476968]\n",
            "[Epoch 24/200] [Batch 2/18] [D loss: 0.252998] [G loss: 0.366363] [info loss: 1.494589]\n",
            "[Epoch 24/200] [Batch 3/18] [D loss: 0.255196] [G loss: 0.322660] [info loss: 1.479707]\n",
            "[Epoch 24/200] [Batch 4/18] [D loss: 0.253404] [G loss: 0.293376] [info loss: 1.499038]\n",
            "[Epoch 24/200] [Batch 5/18] [D loss: 0.236180] [G loss: 0.340777] [info loss: 1.489599]\n",
            "[Epoch 24/200] [Batch 6/18] [D loss: 0.262049] [G loss: 0.355482] [info loss: 1.495504]\n",
            "[Epoch 24/200] [Batch 7/18] [D loss: 0.238316] [G loss: 0.334734] [info loss: 1.496774]\n",
            "[Epoch 24/200] [Batch 8/18] [D loss: 0.243266] [G loss: 0.350982] [info loss: 1.480664]\n",
            "[Epoch 24/200] [Batch 9/18] [D loss: 0.241483] [G loss: 0.354572] [info loss: 1.506613]\n",
            "[Epoch 24/200] [Batch 10/18] [D loss: 0.230756] [G loss: 0.344475] [info loss: 1.484387]\n",
            "[Epoch 24/200] [Batch 11/18] [D loss: 0.269185] [G loss: 0.362281] [info loss: 1.485872]\n",
            "[Epoch 24/200] [Batch 12/18] [D loss: 0.242521] [G loss: 0.362316] [info loss: 1.483455]\n",
            "[Epoch 24/200] [Batch 13/18] [D loss: 0.241474] [G loss: 0.325256] [info loss: 1.486701]\n",
            "[Epoch 24/200] [Batch 14/18] [D loss: 0.247647] [G loss: 0.398942] [info loss: 1.488370]\n",
            "[Epoch 24/200] [Batch 15/18] [D loss: 0.231543] [G loss: 0.359532] [info loss: 1.514566]\n",
            "[Epoch 24/200] [Batch 16/18] [D loss: 0.237575] [G loss: 0.348366] [info loss: 1.491094]\n",
            "[Epoch 24/200] [Batch 17/18] [D loss: 0.254322] [G loss: 0.301059] [info loss: 1.497882]\n",
            "[Epoch 25/200] [Batch 0/18] [D loss: 0.256519] [G loss: 0.387196] [info loss: 1.487571]\n",
            "[Epoch 25/200] [Batch 1/18] [D loss: 0.229542] [G loss: 0.436732] [info loss: 1.492807]\n",
            "[Epoch 25/200] [Batch 2/18] [D loss: 0.256847] [G loss: 0.394647] [info loss: 1.493693]\n",
            "[Epoch 25/200] [Batch 3/18] [D loss: 0.234133] [G loss: 0.296866] [info loss: 1.486218]\n",
            "[Epoch 25/200] [Batch 4/18] [D loss: 0.254524] [G loss: 0.365122] [info loss: 1.494600]\n",
            "[Epoch 25/200] [Batch 5/18] [D loss: 0.262260] [G loss: 0.303843] [info loss: 1.480562]\n",
            "[Epoch 25/200] [Batch 6/18] [D loss: 0.256900] [G loss: 0.334565] [info loss: 1.482828]\n",
            "[Epoch 25/200] [Batch 7/18] [D loss: 0.233185] [G loss: 0.358248] [info loss: 1.486425]\n",
            "[Epoch 25/200] [Batch 8/18] [D loss: 0.232397] [G loss: 0.363119] [info loss: 1.474366]\n",
            "[Epoch 25/200] [Batch 9/18] [D loss: 0.268425] [G loss: 0.294580] [info loss: 1.494318]\n",
            "[Epoch 25/200] [Batch 10/18] [D loss: 0.243562] [G loss: 0.342259] [info loss: 1.480551]\n",
            "[Epoch 25/200] [Batch 11/18] [D loss: 0.279023] [G loss: 0.364113] [info loss: 1.495300]\n",
            "[Epoch 25/200] [Batch 12/18] [D loss: 0.263530] [G loss: 0.340052] [info loss: 1.496165]\n",
            "[Epoch 25/200] [Batch 13/18] [D loss: 0.224918] [G loss: 0.387022] [info loss: 1.483072]\n",
            "[Epoch 25/200] [Batch 14/18] [D loss: 0.269977] [G loss: 0.333030] [info loss: 1.482839]\n",
            "[Epoch 25/200] [Batch 15/18] [D loss: 0.241848] [G loss: 0.308234] [info loss: 1.489990]\n",
            "[Epoch 25/200] [Batch 16/18] [D loss: 0.250397] [G loss: 0.312495] [info loss: 1.489125]\n",
            "[Epoch 25/200] [Batch 17/18] [D loss: 0.256466] [G loss: 0.399247] [info loss: 1.499111]\n",
            "[Epoch 26/200] [Batch 0/18] [D loss: 0.248566] [G loss: 0.361104] [info loss: 1.480439]\n",
            "[Epoch 26/200] [Batch 1/18] [D loss: 0.234999] [G loss: 0.351774] [info loss: 1.479764]\n",
            "[Epoch 26/200] [Batch 2/18] [D loss: 0.251705] [G loss: 0.337791] [info loss: 1.478559]\n",
            "[Epoch 26/200] [Batch 3/18] [D loss: 0.221700] [G loss: 0.343951] [info loss: 1.484649]\n",
            "[Epoch 26/200] [Batch 4/18] [D loss: 0.248163] [G loss: 0.332752] [info loss: 1.493162]\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(\"images/static/\", exist_ok=True)\n",
        "os.makedirs(\"images/varying_c1/\", exist_ok=True)\n",
        "os.makedirs(\"images/varying_c2/\", exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=62, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--code_dim\", type=int, default=2, help=\"latent code\")\n",
        "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
        "\n",
        "# Parse arguments\n",
        "opt, unknown = parser.parse_known_args()\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def to_categorical(y, num_columns):\n",
        "    \"\"\"Returns one-hot encoded Variable\"\"\"\n",
        "    y_cat = np.zeros((y.shape[0], num_columns))\n",
        "    y_cat[range(y.shape[0]), y] = 1.0\n",
        "\n",
        "    return Variable(FloatTensor(y_cat))\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        input_dim = opt.latent_dim + opt.n_classes + opt.code_dim\n",
        "\n",
        "        self.init_size = opt.img_size // 4  # Initial size before upsampling\n",
        "        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels, code):\n",
        "        gen_input = torch.cat((noise, labels, code), -1)\n",
        "        out = self.l1(gen_input)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels, num_classes, code_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, bn=True):\n",
        "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
        "            if bn:\n",
        "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "            return block\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            *discriminator_block(img_channels, 16, bn=False),\n",
        "            *discriminator_block(16, 32),\n",
        "            *discriminator_block(32, 64),\n",
        "            *discriminator_block(64, 128),\n",
        "        )\n",
        "\n",
        "        # The height and width of downsampled image\n",
        "        ds_size = opt.img_size // 2 ** 4\n",
        "\n",
        "        # Output layers\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
        "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, num_classes), nn.Softmax())\n",
        "        self.latent_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, code_dim))\n",
        "\n",
        "    def forward(self, img):\n",
        "        # If the image has only one channel, duplicate it to create three channels\n",
        "        if img.size(1) == 1:\n",
        "            img = torch.cat([img, img, img], dim=1)\n",
        "\n",
        "        out = self.conv_blocks(img)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        label = self.aux_layer(out)\n",
        "        latent_code = self.latent_layer(out)\n",
        "\n",
        "        return validity, label, latent_code\n",
        "\n",
        "\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "categorical_loss = torch.nn.CrossEntropyLoss()\n",
        "continuous_loss = torch.nn.MSELoss()\n",
        "\n",
        "# Loss weights\n",
        "lambda_cat = 1\n",
        "lambda_con = 0.1\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator(img_channels=3, num_classes=10, code_dim=2)\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    categorical_loss.cuda()\n",
        "    continuous_loss.cuda()\n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((opt.img_size, opt.img_size)),  # Corrected: pass a tuple of height and width\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust normalization if needed\n",
        "])\n",
        "\n",
        "# DataLoader for your custom dataset\n",
        "custom_dataset = datasets.ImageFolder(\n",
        "    root=\"/content/drive/MyDrive/Bone Break Classification\",  # Path to your dataset\n",
        "    transform=transform,  # Apply the defined transformations\n",
        ")\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    custom_dataset,\n",
        "    batch_size=opt.batch_size,  # Set batch size\n",
        "    shuffle=True,  # Shuffle the data\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_info = torch.optim.Adam(\n",
        "    itertools.chain(generator.parameters(), discriminator.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2)\n",
        ")\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "\n",
        "# Static generator inputs for sampling\n",
        "static_z = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.latent_dim))))\n",
        "static_label = to_categorical(\n",
        "    np.array([num for _ in range(opt.n_classes) for num in range(opt.n_classes)]), num_columns=opt.n_classes\n",
        ")\n",
        "static_code = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.code_dim))))\n",
        "\n",
        "\n",
        "def sample_image(n_row, batches_done):\n",
        "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
        "    # Static sample\n",
        "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
        "    static_sample = generator(z, static_label, static_code)\n",
        "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "\n",
        "    # Get varied c1 and c2\n",
        "    zeros = np.zeros((n_row ** 2, 1))\n",
        "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
        "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
        "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
        "    sample1 = generator(static_z, static_label, c1)\n",
        "    sample2 = generator(static_z, static_label, c2)\n",
        "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, labels) in enumerate(dataloader):\n",
        "\n",
        "        batch_size = imgs.shape[0]\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(FloatTensor))\n",
        "        labels = to_categorical(labels.numpy(), num_columns=opt.n_classes)\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise and labels as generator input\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
        "        label_input = to_categorical(np.random.randint(0, opt.n_classes, batch_size), num_columns=opt.n_classes)\n",
        "        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z, label_input, code_input)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        validity, _, _ = discriminator(gen_imgs)\n",
        "        g_loss = adversarial_loss(validity, valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss for real images\n",
        "        real_pred, _, _ = discriminator(real_imgs)\n",
        "        d_real_loss = adversarial_loss(real_pred, valid)\n",
        "\n",
        "        # Loss for fake images\n",
        "        fake_pred, _, _ = discriminator(gen_imgs.detach())\n",
        "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ------------------\n",
        "        # Information Loss\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_info.zero_grad()\n",
        "\n",
        "        # Sample labels\n",
        "        sampled_labels = np.random.randint(0, opt.n_classes, batch_size)\n",
        "\n",
        "        # Ground truth labels\n",
        "        gt_labels = Variable(LongTensor(sampled_labels), requires_grad=False)\n",
        "\n",
        "        # Sample noise, labels and code as generator input\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
        "        label_input = to_categorical(sampled_labels, num_columns=opt.n_classes)\n",
        "        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))\n",
        "\n",
        "        gen_imgs = generator(z, label_input, code_input)\n",
        "        _, pred_label, pred_code = discriminator(gen_imgs)\n",
        "\n",
        "        info_loss = lambda_cat * categorical_loss(pred_label, gt_labels) + lambda_con * continuous_loss(\n",
        "            pred_code, code_input\n",
        "        )\n",
        "\n",
        "        info_loss.backward()\n",
        "        optimizer_info.step()\n",
        "\n",
        "        # --------------\n",
        "        # Log Progress\n",
        "        # --------------\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [info loss: %f]\"\n",
        "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), info_loss.item())\n",
        "        )\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            sample_image(n_row=10, batches_done=batches_done)"
      ]
    }
  ]
}